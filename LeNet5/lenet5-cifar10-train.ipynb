{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-12T15:25:24.230933Z","iopub.execute_input":"2022-04-12T15:25:24.231158Z","iopub.status.idle":"2022-04-12T15:25:24.258158Z","shell.execute_reply.started":"2022-04-12T15:25:24.231097Z","shell.execute_reply":"2022-04-12T15:25:24.257534Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Common Includes","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\nimport tensorflow_datasets as tfds\nprint(\"TF version:\", tf.__version__)\nprint(\"Hub version:\", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n!lscpu |grep 'Model name'\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-04-12T15:25:24.259541Z","iopub.execute_input":"2022-04-12T15:25:24.259776Z","iopub.status.idle":"2022-04-12T15:25:32.207420Z","shell.execute_reply.started":"2022-04-12T15:25:24.259744Z","shell.execute_reply":"2022-04-12T15:25:32.206577Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Importing the cifar10 dataset and creating an input pipeline\n\nDownloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB)","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nMAX_BRIGHT_DELTA = 0.1\n\n(ds_train, ds_test), ds_info = tfds.load(\n    name=\"cifar10\",\n    split=[\"train\", \"test\"],\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True,\n)\nprint(ds_info)\nfig = tfds.show_examples(ds_train, ds_info, rows=4, cols=4)\n\ndef normalize_image(image, label):\n    return tf.cast(x=image, dtype=tf.float32)/255.0, label\n    return image, label\ndef rgb_to_gray(image, label):\n    image = tf.image.rgb_to_grayscale(image)\n    return image, label\ndef augment(image, label):\n    image = tf.image.random_brightness(image, max_delta=MAX_BRIGHT_DELTA)\n    #image = tf.image.random_contrast(image, lower=CONTRAST_LOWER, upper=CONTRAST_UPPER) #bad\n    image = tf.image.random_flip_left_right(image) #50%\n    return image, label\n\nds_train = ds_train.map(map_func=rgb_to_gray, num_parallel_calls=AUTOTUNE)\nds_train = ds_train.map(map_func=normalize_image, num_parallel_calls=AUTOTUNE)\nds_train = ds_train.map(map_func=augment, num_parallel_calls=AUTOTUNE)\nds_train = ds_train.shuffle(buffer_size=BUFFER_SIZE)\nds_train = ds_train.batch(batch_size=BATCH_SIZE)\nds_train = ds_train.prefetch(buffer_size=AUTOTUNE)\n\nds_test = ds_test.map(map_func=rgb_to_gray, num_parallel_calls=AUTOTUNE)\nds_test = ds_test.map(map_func=normalize_image, num_parallel_calls=AUTOTUNE)\nds_test = ds_test.batch(batch_size=BATCH_SIZE)\nds_test = ds_test.prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T15:27:41.238031Z","iopub.execute_input":"2022-04-12T15:27:41.238302Z","iopub.status.idle":"2022-04-12T15:27:42.167998Z","shell.execute_reply.started":"2022-04-12T15:27:41.238274Z","shell.execute_reply":"2022-04-12T15:27:42.167290Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Creating the LeNet-5 model","metadata":{}},{"cell_type":"code","source":"INPUT_SHAPE = (32,32,1)\nLEARNING_RATE = 0.001\n\n\nmodel = keras.Sequential([\n    layers.Input(shape=INPUT_SHAPE),\n    layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding=\"valid\"),\n    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n    layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'),\n    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n    layers.Flatten(),\n    layers.Dense(units=120, activation='tanh'),\n    layers.Dense(units=84, activation='tanh'),\n    layers.Dense(units=10)\n])\n\nEPOCHS = 20\n\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n    metrics=['accuracy']\n)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T15:27:35.570845Z","iopub.execute_input":"2022-04-12T15:27:35.571516Z","iopub.status.idle":"2022-04-12T15:27:35.634574Z","shell.execute_reply.started":"2022-04-12T15:27:35.571479Z","shell.execute_reply":"2022-04-12T15:27:35.633621Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Calculating the FLOPs of the models","metadata":{}},{"cell_type":"code","source":"from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n\ndef get_flops(model):\n    concrete = tf.function(lambda inputs: model(inputs))\n    concrete_func = concrete.get_concrete_function(\n        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n    with tf.Graph().as_default() as graph:\n        tf.graph_util.import_graph_def(graph_def, name='')\n        run_meta = tf.compat.v1.RunMetadata()\n        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n        return flops.total_float_ops\n    \n# The //2 is necessary since `profile` counts multiply and accumulate\n# as two flops, here we report the total number of multiply accumulate ops    \nprint(\"The FLOPs is:{}\".format(get_flops(model)//2) ,flush=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:33:54.930504Z","iopub.execute_input":"2022-04-12T13:33:54.930861Z","iopub.status.idle":"2022-04-12T13:33:55.076066Z","shell.execute_reply.started":"2022-04-12T13:33:54.930778Z","shell.execute_reply":"2022-04-12T13:33:55.074958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training the model (15s per Epoch)","metadata":{}},{"cell_type":"code","source":"model.fit(ds_train, epochs=EPOCHS, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:35:51.239958Z","iopub.execute_input":"2022-04-12T13:35:51.240296Z","iopub.status.idle":"2022-04-12T13:41:29.103877Z","shell.execute_reply.started":"2022-04-12T13:35:51.240265Z","shell.execute_reply":"2022-04-12T13:41:29.102731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluating the Model","metadata":{}},{"cell_type":"code","source":"model.evaluate(ds_test, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:42:10.760495Z","iopub.execute_input":"2022-04-12T13:42:10.760774Z","iopub.status.idle":"2022-04-12T13:42:12.212008Z","shell.execute_reply.started":"2022-04-12T13:42:10.760741Z","shell.execute_reply":"2022-04-12T13:42:12.210702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate the model with batch_size=1, to get the single batch average (1/100) inference time, on cpu","metadata":{}},{"cell_type":"code","source":"(ds_test_single), ds_info = tfds.load(\n    name=\"cifar10\",\n    split=\"test[0:100]\",\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True,\n)\n\nds_test_single = ds_test_single.map(map_func=rgb_to_gray, num_parallel_calls=AUTOTUNE)\nds_test_single = ds_test_single.map(map_func=normalize_image, num_parallel_calls=AUTOTUNE)\nds_test_single = ds_test_single.batch(batch_size=1)\nds_test_single = ds_test_single.prefetch(buffer_size=AUTOTUNE)\n\nwith tf.device('/cpu:0'):\n    model.evaluate(ds_test_single, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T15:27:54.708059Z","iopub.execute_input":"2022-04-12T15:27:54.708319Z","iopub.status.idle":"2022-04-12T15:27:55.147050Z","shell.execute_reply.started":"2022-04-12T15:27:54.708292Z","shell.execute_reply":"2022-04-12T15:27:55.146344Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Save the model, zip it and remove the folder to save space","metadata":{}},{"cell_type":"code","source":"Model_Name = \"LeNet5_Cifar10_47pct_0_5MF\"\nos.environ[\"MODELNAME\"] = Model_Name\nos.environ[\"MODELNAMEZIP\"] = Model_Name + \".zip\"\nmodel.save(\"./\" + Model_Name)\n!zip -r ./$MODELNAMEZIP ./$MODELNAME\n!rm -rf ./$MODELNAME","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:42:37.739892Z","iopub.execute_input":"2022-04-12T13:42:37.740175Z","iopub.status.idle":"2022-04-12T13:42:40.82316Z","shell.execute_reply.started":"2022-04-12T13:42:37.740143Z","shell.execute_reply":"2022-04-12T13:42:40.821944Z"},"trusted":true},"execution_count":null,"outputs":[]}]}